name: Generate DataQuality Baseline (one-off)

on:
  workflow_dispatch:
    inputs:
      dataset_s3:
        description: "학습 데이터 S3 URI (CSV/JSONL)"
        required: true
      baseline_prefix:
        description: "Baseline 저장 S3 prefix (끝에 /)"
        required: true
      instance_type:
        description: "Processing 인스턴스 타입"
        required: false
        default: "ml.m5.xlarge"

  workflow_call:                      
    inputs:
      dataset_s3:
        required: true
        type: string
      baseline_prefix:
        required: true
        type: string
      instance_type:
        required: false
        type: string
        default: "ml.m5.xlarge"

permissions:
  id-token: write
  contents: read

jobs:
  gen-baseline:
    runs-on: ubuntu-latest
    steps:
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Run ProcessingJob for baseline (robust JSON)
        run: |
          set -Eeuo pipefail

          IMAGE="${{ vars.MONITOR_IMAGE_URI }}"
          ROLE="${{ secrets.SAGEMAKER_PROCESSING_ROLE_ARN || vars.SAGEMAKER_PROCESSING_ROLE_ARN }}"
          DATASET="${{ inputs.dataset_s3 }}"
          BASELINE="${{ inputs.baseline_prefix }}"
          ITYPE="${{ inputs.instance_type }}"
          JOB_NAME="dq-baseline-$(date +%Y%m%d-%H%M%S)"

          # JSON 템플릿 생성 (정확한 JSON)
          cat > /tmp/job.json <<'JSON'
          {
            "ProcessingJobName": "__JOB_NAME__",
            "AppSpecification": {
              "ImageUri": "__IMAGE__",
              "ContainerArguments": ["baseline"]
            },
            "RoleArn": "__ROLE__",
            "ProcessingResources": {
              "ClusterConfig": {
                "InstanceCount": 1,
                "InstanceType": "__ITYPE__",
                "VolumeSizeInGB": 30
              }
            },
            "ProcessingInputs": [
              {
                "InputName": "dataset",
                "S3Input": {
                  "S3Uri": "__DATASET__",
                  "LocalPath": "/opt/ml/processing/input",
                  "S3DataType": "S3Prefix",
                  "S3InputMode": "File"
                }
              }
            ],
            "ProcessingOutputConfig": {
              "Outputs": [
                {
                  "OutputName": "baseline",
                  "S3Output": {
                    "S3Uri": "__BASELINE__dataquality/",
                    "LocalPath": "/opt/ml/processing/output",
                    "S3UploadMode": "EndOfJob"
                  }
                }
              ]
            },
            "StoppingCondition": { "MaxRuntimeInSeconds": 3600 }
            }
            JSON

            # 변수 치환
            sed -i "s|__JOB_NAME__|$JOB_NAME|g"   /tmp/job.json
            sed -i "s|__IMAGE__|$IMAGE|g"         /tmp/job.json
            sed -i "s|__ROLE__|$ROLE|g"           /tmp/job.json
            sed -i "s|__ITYPE__|$ITYPE|g"         /tmp/job.json
            sed -i "s|__DATASET__|$DATASET|g"     /tmp/job.json
            sed -i "s|__BASELINE__|$BASELINE|g"   /tmp/job.json

            cat /tmp/job.json

            # 실행
            aws sagemaker create-processing-job --cli-input-json file:///tmp/job.json

            echo "Started ProcessingJob: $JOB_NAME"

