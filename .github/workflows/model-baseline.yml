name: Generate DataQuality Baseline (one-off)

on:
  workflow_dispatch:
    inputs:
      dataset_s3:
        description: "학습 데이터 S3 URI (CSV/JSONL)"
        required: true
      baseline_prefix:
        description: "Baseline 저장 S3 prefix (끝에 /)"
        required: true
      instance_type:
        description: "Processing 인스턴스 타입"
        required: false
        default: "ml.m5.xlarge"
      dataset_type:
        description: "데이터 형식(csv | json)"
        required: false
        default: "csv"

  workflow_call:                      
    inputs:
      dataset_s3:
        required: true
        type: string
      baseline_prefix:
        required: true
        type: string
      instance_type:
        required: false
        type: string
        default: "ml.m5.xlarge"
      dataset_type:
        required: false
        type: string
        default: "csv"

permissions:
  id-token: write
  contents: read

jobs:
  baseline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Install sagemaker SDK
        run: |
          python3 -m pip install --upgrade pip
          pip install "sagemaker>=2.249.0" boto3

      - name: Generate baseline with SageMaker SDK
        env:
          # Processing Job이 사용할 Role (권한: s3 Get/Put, logs, sagemaker:CreateProcessingJob 등)
          SAGEMAKER_PROCESSING_ROLE_ARN: ${{ secrets.SAGEMAKER_PROCESSING_ROLE_ARN || vars.SAGEMAKER_PROCESSING_ROLE_ARN }}
          AWS_REGION: ${{ vars.AWS_REGION }}
          DATASET_S3: ${{ inputs.dataset_s3 }}
          BASELINE_PREFIX: ${{ inputs.baseline_prefix }}
          INSTANCE_TYPE: ${{ inputs.instance_type }}
          DATASET_TYPE: ${{ inputs.dataset_type }}
        run: |
          python - <<'PY'
          import os, sys
          from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat

          role_arn = os.environ.get("SAGEMAKER_PROCESSING_ROLE_ARN")
          region   = os.environ["AWS_REGION"]
          dataset  = os.environ["DATASET_S3"]
          outpref  = os.environ["BASELINE_PREFIX"]
          itype    = os.environ.get("INSTANCE_TYPE","ml.m5.xlarge")
          dtype    = os.environ.get("DATASET_TYPE","csv").lower()

          if not role_arn:
            sys.exit("Missing SAGEMAKER_PROCESSING_ROLE_ARN (vars or secrets)")
          if not (dataset.startswith("s3://")):
            sys.exit("dataset_s3 must be an S3 URI")
          if not (outpref.startswith("s3://") and outpref.endswith("/")):
            sys.exit("baseline_prefix must be an S3 prefix ending with '/'")
          if dtype not in ("csv","json"):
            sys.exit("dataset_type must be 'csv' or 'json'")

          # SDK가 리전에 맞는 analyzer 이미지/환경을 자동 설정
          monitor = DefaultModelMonitor(
              role=role_arn,
              instance_count=1,
              instance_type=itype,
              volume_size_in_gb=30,
              max_runtime_in_seconds=3600,
          )
          
          # SDK가 요구하는 DatasetFormat 생성
          if dtype == "csv":
              dformat = DatasetFormat.csv(header=False)
          elif dtype == "json":
          # JSON Lines 형태일 때
              dformat = DatasetFormat.json(lines=True)
          else:
              raise ValueError("dataset_type must be 'csv' or 'json'")
          
          job = monitor.suggest_baseline(
              baseline_dataset=dataset,
              dataset_format=dformat,
              output_s3_uri=outpref + "dataquality/",
              wait=False  # 비동기로 시작; 성공 시 constraints/statistics 생성
          )

          desc = job.describe()
          print("✅ Started ProcessingJob:", desc.get("ProcessingJobArn"))
          PY

      - name: Echo next steps
        run: |
          echo "Baseline ProcessingJob을 시작했습니다."
          echo "완료되면 ${BASELINE_PREFIX}dataquality/ 아래에 constraints.json, statistics.json 이 생성됩니다."
