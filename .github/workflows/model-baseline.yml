name: Generate DataQuality Baseline (one-off)

on:
  workflow_dispatch:
    inputs:
      dataset_s3:
        description: "학습 데이터 S3 URI (CSV/JSONL)"
        required: true
      baseline_prefix:
        description: "Baseline 저장 S3 prefix (끝에 /)"
        required: true
      instance_type:
        description: "Processing 인스턴스 타입"
        required: false
        default: "ml.m5.xlarge"

  workflow_call:                      
    inputs:
      dataset_s3:
        required: true
        type: string
      baseline_prefix:
        required: true
        type: string
      instance_type:
        required: false
        type: string
        default: "ml.m5.xlarge"

permissions:
  id-token: write
  contents: read

jobs:
  gen-baseline:
    runs-on: ubuntu-latest
    steps:
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Generate baseline (robust)
        env:
          IMAGE:  ${{ vars.MONITOR_IMAGE_URI }}
          ROLE:   ${{ secrets.SAGEMAKER_PROCESSING_ROLE_ARN || vars.SAGEMAKER_PROCESSING_ROLE_ARN }}
          DATASET: ${{ inputs.dataset_s3 }}
          BASELINE: ${{ inputs.baseline_prefix }}
          ITYPE:   ${{ inputs.instance_type }}
        run: |
          set -Eeuo pipefail

          # 0) 필수 env 점검
          for v in IMAGE ROLE DATASET BASELINE ITYPE; do
            if [ -z "${!v:-}" ]; then
              echo "::error::Missing env $v"; exit 1
            fi
          done
          if ! [[ "$BASELINE" =~ /$ ]]; then
            echo "::error::baseline_prefix must end with /"; exit 1
          fi

          JOB_NAME="dq-baseline-$(date +%Y%m%d-%H%M%S)"

          # 1) 정확한 JSON을 jq로 생성 (sed/여분 brace/이스케이프 이슈 제거)
          jq -n \
            --arg job "$JOB_NAME" \
            --arg image "$IMAGE" \
            --arg role "$ROLE" \
            --arg itype "$ITYPE" \
            --arg dataset "$DATASET" \
            --arg base "$BASELINE" \
            '{
              ProcessingJobName: $job,
              AppSpecification: {
              ImageUri: $image,
              ContainerArguments: ["baseline"]
            },
            RoleArn: $role,
            ProcessingResources: {
              ClusterConfig: {
                InstanceCount: 1,
                InstanceType: $itype,
                VolumeSizeInGB: 30
              }
            },
            ProcessingInputs: [
              {
                InputName: "dataset",
                S3Input: {
                  S3Uri: $dataset,
                  LocalPath: "/opt/ml/processing/input",
                  S3DataType: "S3Prefix",
                  S3InputMode: "File"
                }
              }
            ],
            ProcessingOutputConfig: {
              Outputs: [
                {
                  OutputName: "baseline",
                  S3Output: {
                    S3Uri: ($base + "dataquality/"),
                    LocalPath: "/opt/ml/processing/output",
                    S3UploadMode: "EndOfJob"
                  }
                }
              ]
            },
            StoppingCondition: { MaxRuntimeInSeconds: 3600 }
          }' >/tmp/job.json

        echo "=== /tmp/job.json ==="
        cat /tmp/job.json

        # 2) 실행
        aws sagemaker create-processing-job --cli-input-json file:///tmp/job.json
        echo "✅ Started ProcessingJob: $JOB_NAME"

